{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "project_thesis",
   "display_name": "Project-Thesis"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import keras\n",
    "import imgaug\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nConfigurations:\nBACKBONE                       resnet101\nBACKBONE_STRIDES               [4, 8, 16, 32, 64]\nBATCH_SIZE                     1\nBBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\nCOMPUTE_BACKBONE_SHAPE         None\nDETECTION_MAX_INSTANCES        400\nDETECTION_MIN_CONFIDENCE       0.5\nDETECTION_NMS_THRESHOLD        0.4\nFPN_CLASSIF_FC_LAYERS_SIZE     1024\nGPU_COUNT                      1\nGRADIENT_CLIP_NORM             5.0\nIMAGES_PER_GPU                 1\nIMAGE_CHANNEL_COUNT            3\nIMAGE_MAX_DIM                  512\nIMAGE_META_SIZE                14\nIMAGE_MIN_DIM                  512\nIMAGE_MIN_SCALE                0\nIMAGE_RESIZE_MODE              square\nIMAGE_SHAPE                    [512 512   3]\nLEARNING_MOMENTUM              0.9\nLEARNING_RATE                  0.001\nLOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\nMASK_POOL_SIZE                 14\nMASK_SHAPE                     [28, 28]\nMAX_GT_INSTANCES               400\nMEAN_PIXEL                     [165.32, 165.32, 165.32]\nMINI_MASK_SHAPE                (56, 56)\nNAME                           craters\nNUM_CLASSES                    2\nPOOL_SIZE                      7\nPOST_NMS_ROIS_INFERENCE        1000\nPOST_NMS_ROIS_TRAINING         2000\nPRE_NMS_LIMIT                  6000\nROI_POSITIVE_RATIO             0.33\nRPN_ANCHOR_RATIOS              [0.5, 1, 2]\nRPN_ANCHOR_SCALES              (4, 8, 16, 32, 64)\nRPN_ANCHOR_STRIDE              1\nRPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\nRPN_NMS_THRESHOLD              0.7\nRPN_TRAIN_ANCHORS_PER_IMAGE    256\nSTEPS_PER_EPOCH                160\nTOP_DOWN_PYRAMID_SIZE          256\nTRAIN_BN                       False\nTRAIN_ROIS_PER_IMAGE           300\nUSE_MINI_MASK                  True\nUSE_RPN_ROIS                   True\nVALIDATION_STEPS               16\nWEIGHT_DECAY                   0.0001\n\n\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"craters\"\n",
    "    BACKBONE =\"resnet101\" #default resnet101\n",
    "    \n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 1 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (4, 8, 16, 32,64)  # anchor side in pixels, from 4,8\n",
    "    RPN_NMS_THRESHOLD = 0.7 \n",
    "    MEAN_PIXEL = [165.32, 165.32, 165.32]\n",
    "    \n",
    "    #POST_NMS_ROIS_TRAINING = 2000\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    \n",
    "    TRAIN_ROIS_PER_IMAGE = 300\n",
    "    MAX_GT_INSTANCES = 400\n",
    "    DETECTION_MAX_INSTANCES = 400\n",
    "    \n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 160\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 16\n",
    "    # Additional Setting by user\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "    DETECTION_NMS_THRESHOLD  = 0.4\n",
    "    \n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=15):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "def inspect_results(img, bboxs, color=\"red\"):\n",
    "    b=bboxs\n",
    "    image = img.copy()\n",
    "    for i in range(b.shape[0]):\n",
    "        \n",
    "        d1, d2 = b[i,:][1]-b[i,:][3] , b[i,:][0]-b[i,:][2]\n",
    "        d1, d2 = abs(d1), abs(d2)\n",
    "        \n",
    "        r = (d1+d2)//4\n",
    "        x_c, y_c = (b[i,:][1]+b[i,:][3])//2 , (b[i,:][0]+b[i,:][2])//2\n",
    "        \n",
    "        center_coordinates = (x_c, y_c)\n",
    "        radius = r\n",
    "        if color==\"red\":\n",
    "            color = (255,0,0)\n",
    "        elif color==\"green\":\n",
    "            color = (0,255,0)\n",
    "        \n",
    "        thickness= 2\n",
    "        cv2.circle(image, center_coordinates, radius, color, thickness)\n",
    "    \n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def diff_bb(gt_boundingboxes,bounding_boxes):\n",
    "    global x,y, differenza\n",
    "    y = gt_boundingboxes.shape[0]\n",
    "    x = bounding_boxes.shape[0]\n",
    "    \n",
    "    \n",
    "    if y != 0:\n",
    "        differenza = (x-y)/y\n",
    "        return differenza\n",
    "\n",
    "def delinvalidvalues(x):\n",
    "    x = x[x!=0]\n",
    "    x = x[np.isfinite(x)]\n",
    "    return x\n",
    "\n",
    "def runinference(iter):\n",
    "    image_ids = np.random.choice(dataset_test.image_ids, 20)\n",
    "    \n",
    "    global P_ARRAY,R_ARRAY,F1_ARRAY,mAP_ARRAY\n",
    "    \n",
    "    APs = []\n",
    "    P_ARRAY = []\n",
    "    R_ARRAY = []\n",
    "    F1_ARRAY = []\n",
    "    mAP_ARRAY = []\n",
    "    \n",
    "    for i in range(iter):\n",
    "        \n",
    "        for image_id in image_ids:\n",
    "        # Load image and ground truth data\n",
    "            image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset_test, inference_config,\n",
    "                                   image_id)\n",
    "            molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "        # Run object detection\n",
    "            results = model.detect([image], verbose=0)\n",
    "            r = results[0]\n",
    "            # Compute AP\n",
    "            AP, precisions, recalls, overlaps =\\\n",
    "                utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                                 r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "            APs.append(AP)\n",
    "\n",
    "            #print(\"mAP: \", np.mean(APs))\n",
    "\n",
    "            P = np.mean(precisions)\n",
    "            R = np.mean(recalls)\n",
    "            F1 = 2 * P * R / (P + R)\n",
    "            mAP = np.mean(APs)\n",
    "    \n",
    "            P_ARRAY = np.append(P_ARRAY, P)\n",
    "            R_ARRAY = np.append(R_ARRAY, R)\n",
    "            F1_ARRAY = np.append(F1_ARRAY, F1)\n",
    "            mAP_ARRAY = np.append(mAP_ARRAY, mAP)\n",
    "       \n",
    "    P_ARRAY = delinvalidvalues(P_ARRAY)\n",
    "    R_ARRAY = delinvalidvalues(R_ARRAY)\n",
    "    F1_ARRAY = delinvalidvalues(F1_ARRAY)\n",
    "    mAP_ARRAY = delinvalidvalues(mAP_ARRAY)\n",
    "\n",
    "    return P_ARRAY,R_ARRAY,F1_ARRAY,mAP_ARRAY\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(4,4))\n",
    "    image = clahe.apply(image)\n",
    "    #plt.imshow(image, cmap='gray')\n",
    "    image.shape\n",
    "    image3channel = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    #plt.imshow(image3channel)\n",
    "    return image3channel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def match_prep(b, img):\n",
    "    vstack = np.zeros(4)\n",
    "    for i in range(b.shape[0]):\n",
    "        d1, d2 = b[i,:][1]-b[i,:][3] , b[i,:][0]-b[i,:][2]\n",
    "        d1, d2 = abs(d1), abs(d2)\n",
    "            \n",
    "        r = (d1+d2)//4\n",
    "        x_c, y_c = (b[i,:][1]+b[i,:][3])//2 , (b[i,:][0]+b[i,:][2])//2\n",
    "\n",
    "        radius = r\n",
    "        depth = img[y_c, x_c, 0]\n",
    "        brick = [x_c, y_c, radius, depth]\n",
    "        vstack = np.vstack((vstack, brick))\n",
    "    vstack = vstack[1:,:]\n",
    "    return vstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weights Loaded!\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE = 0.85\n",
    "    \n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "inference_config.USE_MINI_MASK= False\n",
    "MODEL_DIR = os.path.abspath(\"\")\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "model.load_weights('/home/sirbastiano/Desktop/Python Projects/Progetto Tesi/Pesi/30-100_norm/mask_rcnn_craters.h5', by_name=True)\n",
    "print(\"Weights Loaded!\")"
   ]
  }
 ]
}